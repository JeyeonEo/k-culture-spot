#!/usr/bin/env python
"""
K-Culture Spot ë°ì´í„° ë¡œë” CLI

ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ê³  DBì— ì ì¬í•˜ëŠ” í†µí•© CLI ë„êµ¬ì…ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/data_loader_cli.py --help
    python scripts/data_loader_cli.py load-json
    python scripts/data_loader_cli.py crawl-tour --keyword "ë„ê¹¨ë¹„"
    python scripts/data_loader_cli.py scrape --drama "ì‚¬ë‘ì˜ ë¶ˆì‹œì°©"
"""

import argparse
import asyncio
import logging
import sys
from pathlib import Path

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from app.core.database import async_session_maker, init_db
from app.models.spot import Category
from app.services.spot_service import SpotService
from app.services.tour_api_crawler import TourAPICrawler
from app.services.web_scraper import KCultureDataScraper
from scripts.load_test_data import load_test_data

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DataLoaderCLI:
    """K-Culture ë°ì´í„° ë¡œë” CLI í´ë˜ìŠ¤."""

    def __init__(self):
        self.tour_crawler = TourAPICrawler()
        self.web_scraper = KCultureDataScraper()

    async def init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        await init_db()
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    async def load_json_data(self, path: str = None) -> tuple[int, int]:
        """
        JSON íŒŒì¼ë“¤ì„ DBì— ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            path: test_db í´ë” ê²½ë¡œ (ê¸°ë³¸ê°’: í”„ë¡œì íŠ¸ ë£¨íŠ¸/test_db)

        Returns:
            (loaded_count, skipped_count)
        """
        await self.init_database()

        test_db_path = Path(path) if path else None

        async with async_session_maker() as db:
            loaded, skipped = await load_test_data(db, test_db_path)

        logger.info(f"JSON ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {loaded}ê°œ ë¡œë“œ, {skipped}ê°œ ìŠ¤í‚µ")
        return loaded, skipped

    async def crawl_tour_api(
        self,
        keywords: list[str],
        category: str = "drama",
        limit: int = 20
    ) -> int:
        """
        Tour APIì—ì„œ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
            category: ì¹´í…Œê³ ë¦¬ (drama, kpop, movie, variety)
            limit: ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            ì €ì¥ëœ ìŠ¤íŒŸ ìˆ˜
        """
        await self.init_database()

        category_enum = getattr(Category, category.upper(), Category.DRAMA)

        saved_count = 0
        async with async_session_maker() as db:
            service = SpotService(db)

            for keyword in keywords:
                logger.info(f"ê²€ìƒ‰ ì¤‘: {keyword}")

                # Search for spots
                items = await self.tour_crawler.search_keyword(
                    keyword,
                    num_of_rows=limit
                )

                for item in items:
                    content_id = item.get("contentid")

                    # Skip if already exists
                    if content_id:
                        existing = await service.get_spot_by_content_id(content_id)
                        if existing:
                            logger.debug(f"ìŠ¤í‚µ: {content_id} ì´ë¯¸ ì¡´ì¬")
                            continue

                    # Get detailed info
                    if content_id:
                        detail = await self.tour_crawler.get_detail_info(content_id)
                        if detail:
                            item.update(detail)

                        intro = await self.tour_crawler.get_detail_intro(content_id)
                        if intro:
                            item.update(intro)

                    # Parse and save
                    spot_data = self.tour_crawler.parse_spot_data(item, category_enum)
                    spot_data.tags = [keyword]

                    try:
                        await service.create_spot(spot_data)
                        saved_count += 1
                        logger.info(f"ì €ì¥ ì™„ë£Œ: {spot_data.name}")
                    except Exception as e:
                        logger.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")

                # Rate limiting
                await asyncio.sleep(0.5)

        logger.info(f"Tour API í¬ë¡¤ë§ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥")
        return saved_count

    async def scrape_web(
        self,
        drama_name: str = None,
        artist_name: str = None,
        limit: int = 10
    ) -> int:
        """
        ì›¹ì—ì„œ ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            drama_name: ë“œë¼ë§ˆ ì´ë¦„
            artist_name: K-pop ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜

        Returns:
            ì €ì¥ëœ ìŠ¤íŒŸ ìˆ˜
        """
        await self.init_database()

        spots = []

        if drama_name:
            logger.info(f"ë“œë¼ë§ˆ ì´¬ì˜ì§€ ìŠ¤í¬ë˜í•‘: {drama_name}")
            spots.extend(
                await self.web_scraper.scrape_drama_locations(drama_name, limit)
            )

        if artist_name:
            logger.info(f"K-pop ê´€ë ¨ ì¥ì†Œ ìŠ¤í¬ë˜í•‘: {artist_name}")
            spots.extend(
                await self.web_scraper.scrape_kpop_venues(artist_name, limit)
            )

        saved_count = 0
        async with async_session_maker() as db:
            service = SpotService(db)

            for spot in spots:
                # Enrich data
                spot = await self.web_scraper.enrich_spot_data(spot)

                try:
                    await service.create_spot(spot)
                    saved_count += 1
                    logger.info(f"ì €ì¥ ì™„ë£Œ: {spot.name}")
                except Exception as e:
                    logger.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")

        logger.info(f"ì›¹ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥")
        return saved_count

    async def show_stats(self):
        """DB í†µê³„ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        await self.init_database()

        async with async_session_maker() as db:
            service = SpotService(db)
            total = await service.count_spots()

            # Get counts by category
            category_counts = {}
            for category in Category:
                spots, count = await service.get_spots_by_category(
                    category, page=1, page_size=1
                )
                category_counts[category.value] = count

        print("\n" + "=" * 50)
        print("ğŸ“Š K-Culture Spot ë°ì´í„°ë² ì´ìŠ¤ í†µê³„")
        print("=" * 50)
        print(f"\nì´ ìŠ¤íŒŸ ìˆ˜: {total}ê°œ")
        print("\nì¹´í…Œê³ ë¦¬ë³„:")
        for category, count in category_counts.items():
            print(f"  - {category}: {count}ê°œ")
        print("=" * 50)


def create_parser() -> argparse.ArgumentParser:
    """CLI íŒŒì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(
        description="K-Culture Spot ë°ì´í„° ë¡œë”",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # JSON íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
  python scripts/data_loader_cli.py load-json

  # Tour APIì—ì„œ ë“œë¼ë§ˆ ì´¬ì˜ì§€ í¬ë¡¤ë§
  python scripts/data_loader_cli.py crawl-tour -k "ë„ê¹¨ë¹„" -k "ì‚¬ë‘ì˜ ë¶ˆì‹œì°©"

  # ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ë“œë¼ë§ˆ ì´¬ì˜ì§€ ìˆ˜ì§‘
  python scripts/data_loader_cli.py scrape --drama "ì´íƒœì› í´ë¼ì“°"

  # K-pop ê´€ë ¨ ì¥ì†Œ ìŠ¤í¬ë˜í•‘
  python scripts/data_loader_cli.py scrape --artist "BTS"

  # DB í†µê³„ í™•ì¸
  python scripts/data_loader_cli.py stats
        """
    )

    subparsers = parser.add_subparsers(dest="command", help="ëª…ë ¹ì–´")

    # load-json command
    json_parser = subparsers.add_parser(
        "load-json",
        help="JSON íŒŒì¼ë“¤ì„ DBì— ë¡œë“œ"
    )
    json_parser.add_argument(
        "-p", "--path",
        help="test_db í´ë” ê²½ë¡œ (ê¸°ë³¸ê°’: í”„ë¡œì íŠ¸ ë£¨íŠ¸/test_db)"
    )

    # crawl-tour command
    tour_parser = subparsers.add_parser(
        "crawl-tour",
        help="Tour APIì—ì„œ ë°ì´í„° í¬ë¡¤ë§"
    )
    tour_parser.add_argument(
        "-k", "--keyword",
        action="append",
        dest="keywords",
        required=True,
        help="ê²€ìƒ‰ í‚¤ì›Œë“œ (ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥)"
    )
    tour_parser.add_argument(
        "-c", "--category",
        default="drama",
        choices=["drama", "kpop", "movie", "variety"],
        help="ì¹´í…Œê³ ë¦¬ (ê¸°ë³¸ê°’: drama)"
    )
    tour_parser.add_argument(
        "-l", "--limit",
        type=int,
        default=20,
        help="ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 20)"
    )

    # scrape command
    scrape_parser = subparsers.add_parser(
        "scrape",
        help="ì›¹ì—ì„œ ë°ì´í„° ìŠ¤í¬ë˜í•‘"
    )
    scrape_parser.add_argument(
        "--drama",
        dest="drama_name",
        help="ë“œë¼ë§ˆ ì´ë¦„"
    )
    scrape_parser.add_argument(
        "--artist",
        dest="artist_name",
        help="K-pop ì•„í‹°ìŠ¤íŠ¸ ì´ë¦„"
    )
    scrape_parser.add_argument(
        "-l", "--limit",
        type=int,
        default=10,
        help="ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 10)"
    )

    # stats command
    subparsers.add_parser("stats", help="DB í†µê³„ í™•ì¸")

    # all command (load all data)
    all_parser = subparsers.add_parser(
        "all",
        help="ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"
    )
    all_parser.add_argument(
        "--skip-tour",
        action="store_true",
        help="Tour API í¬ë¡¤ë§ ìŠ¤í‚µ"
    )

    return parser


async def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = create_parser()
    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    cli = DataLoaderCLI()

    print("\n" + "=" * 50)
    print("ğŸ‡°ğŸ‡· K-Culture Spot ë°ì´í„° ë¡œë”")
    print("=" * 50)

    if args.command == "load-json":
        await cli.load_json_data(args.path)

    elif args.command == "crawl-tour":
        await cli.crawl_tour_api(
            keywords=args.keywords,
            category=args.category,
            limit=args.limit
        )

    elif args.command == "scrape":
        if not args.drama_name and not args.artist_name:
            print("ì˜¤ë¥˜: --drama ë˜ëŠ” --artist ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            return
        await cli.scrape_web(
            drama_name=args.drama_name,
            artist_name=args.artist_name,
            limit=args.limit
        )

    elif args.command == "stats":
        await cli.show_stats()

    elif args.command == "all":
        print("\nğŸ“‚ JSON ë°ì´í„° ë¡œë“œ ì¤‘...")
        await cli.load_json_data()

        if not args.skip_tour:
            print("\nğŸŒ Tour API í¬ë¡¤ë§ ì¤‘...")
            default_keywords = [
                "ë„ê¹¨ë¹„", "ì‚¬ë‘ì˜ ë¶ˆì‹œì°©", "ì´íƒœì› í´ë¼ì“°",
                "BTS", "BLACKPINK"
            ]
            await cli.crawl_tour_api(default_keywords)

        await cli.show_stats()

    print("\nâœ… ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(main())
